{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Twitter data\n",
    "\n",
    "This notebook is to pre-process the Twitter data for topic modeling and sentiment analysis.\n",
    "\n",
    "Data cleaning:\n",
    "- Limit to tweets in English\n",
    "- Transform to all lowercase\n",
    "- Remove URLs and HTML reference characters\n",
    "- Remove placeholders\n",
    "- Remove non-letter characters\n",
    "- Removes unnecessary columns\n",
    "- Change date to pd.datetime\n",
    "- Remove stop words? [This article](https://www.aclweb.org/anthology/L14-1265/) says that removing stop words might affect sentiment analysis performance\n",
    "- Stem/lemmatize the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data\n",
    "def preprocess_data(df):\n",
    "    '''\n",
    "    Pre-processes the data as described above\n",
    "    '''\n",
    "    processed_df = df.loc[df.lang == \"en\", :].copy()\n",
    "    columns_to_keep = [\n",
    "        'date', 'content', 'url', 'coordinates', 'place', 'id', 'username', \n",
    "        'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
    "        'conversationId', 'retweetedTweet', 'quotedTweet', 'outlinks', \n",
    "        'tcooutlinks', 'media', 'mentionedUsers'\n",
    "    ]\n",
    "    \n",
    "    processed_df = processed_df[columns_to_keep]\n",
    "    processed_df.date = pd.to_datetime(processed_df.date[:10], yearfirst=True, format=\"%Y-%m-%d\")\n",
    "    processed_df.content = processed_df.content.str.lower()\n",
    "    processed_df.content = processed_df.content.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "    processed_df.content = processed_df.content.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "    processed_df.content = processed_df.content.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "    processed_df.content = processed_df.content.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "    processed_df.content = processed_df.content.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "    processed_df.content = processed_df.content.apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n",
    "    \n",
    "    processed_df['tokens'] = processed_df['content'].apply(tknzr.tokenize)\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: tweets_9.csv\n",
      "Working on: tweets_8.csv\n",
      "Working on: tweets_29.csv\n",
      "Working on: tweets_15.csv\n",
      "Working on: tweets_14.csv\n",
      "Working on: tweets_28.csv\n",
      "Working on: tweets_16.csv\n",
      "Working on: tweets_17.csv\n",
      "Working on: tweets_13.csv\n",
      "Working on: tweets_12.csv\n",
      "Working on: tweets_10.csv\n",
      "Working on: tweets_38.csv\n",
      "Working on: tweets_39.csv\n",
      "Working on: tweets_11.csv\n",
      "Working on: tweets_20.csv\n",
      "Working on: tweets_34.csv\n",
      "Working on: tweets_35.csv\n",
      "Working on: tweets_21.csv\n",
      "Working on: tweets_37.csv\n",
      "Working on: tweets_23.csv\n",
      "Working on: tweets_22.csv\n",
      "Working on: tweets_36.csv\n",
      "Working on: tweets_32.csv\n",
      "Working on: tweets_26.csv\n",
      "Working on: tweets_27.csv\n",
      "Working on: tweets_33.csv\n",
      "Working on: tweets_25.csv\n",
      "Working on: tweets_31.csv\n",
      "Working on: tweets_19.csv\n",
      "Working on: tweets_18.csv\n",
      "Working on: tweets_30.csv\n",
      "Working on: tweets_24.csv\n",
      "Working on: tweets_3.csv\n",
      "Working on: tweets_2.csv\n",
      "Working on: tweets_40.csv\n",
      "Working on: tweets_1.csv\n",
      "Working on: tweets_5.csv\n",
      "Working on: tweets_4.csv\n",
      "Working on: tweets_6.csv\n",
      "Working on: tweets_7.csv\n"
     ]
    }
   ],
   "source": [
    "# Load Twitter data\n",
    "path, dirs, files = next(os.walk(\"data/\"))\n",
    "df_list = []\n",
    "  \n",
    "# Pre-process\n",
    "for file in files:\n",
    "    print(\"Working on:\", file)\n",
    "    raw_df = pd.read_csv(\"data/\" + file)\n",
    "    clean_df = preprocess_data(raw_df)\n",
    "    df_list.append(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the dataframes\n",
    "main_df = pd.concat([df for df in df_list], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246014, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
